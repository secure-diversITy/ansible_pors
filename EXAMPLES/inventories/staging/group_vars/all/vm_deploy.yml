---
####################################################################################################################
#
# ALL OUTCOMMENTED KEY:VALUES (IF ANY) YOU SEE HERE ARE THE DEFINED DEFAULT VALUES
# i.e. there is no need to enable them if you like the shown default value
#
####################################################################################################################

#####################################################################################################################
# Deploy VM's based on a given template.
# Check documentation.
#####################################################################################################################

##############################
# VMware vSphere definitions #
##############################
vsphere:
    # if set to true no RAM backup swap (vswp) will be created
    # use this with care and avoid any over-provisioning of RAM then!
    # if not set or set to false vmem will act as usual.
    disablevmem: true

    # VMware will try to boot over PXE on each boot which is usually useless
    # set this to True to disable PXE boot when deploying a new VM
    disablepxeboot: True

    # set "True" to enable CPU + RAM hotplug for the deployed VMs
    enable_hotplug: True
 
    host:
       # you can specify as much vsphere centers as you wish.
       # vsphere-hostname-01 is just an example any should be replaced by the hostname
       # of your real FQDN (i.e. without domain).
       vsphere-hostname-01:
            dest: "full-fqdn"
            validate_certs: "no"
            user: "{{ vault_vsphere_user }}"
            password: "{{ vault_vsphere_password }}"
            ansible_template_name: "rhel_ansible_template"

       #vsphere-hostname-02:
           #dest: "full-fqdn"
            #...

##############################
# Google GCP definitions     #
##############################
googlecp:
    project:
        # replace <prod_site1_us> with a freely choosen name and "<my-project-id>" with the REAL project-id!
        # while you're free to choose any name it might be a good idea to choose something like:
        # <environment>_<splunksite>_<location>
        # rule of thumb is to split by service account, project id, metadata, region, zone, source image
        staging_site1_us:
            id: <my-project-id>
            cred_kind: serviceaccount
            cred_file: $HOME/.gcp.json
            zone: "us-central1-a"
            region: "us-central1"
            scopes:
                - https://www.googleapis.com/auth/compute
            source_image: "projects/templates/global/images/sles12-sp4-latest"
            metadata_service: "splunk"
            metadata_landscape: "staging"
            subnet: "<network-subnet-name-as-defined-in-GCP>"

        #staging_site2_eu:
        #    id: <my-project-id>
        #    ....

##############################
# Proxmox PVE definitions    #
##############################
proxmox:
    # datacenter node
    dc: 
        host: "host-FQDN"
        validate_certs: yes
    node:
        # replace <prod_site1_de> with the actual node name of your Proxmox VE
        # while you're free to choose any name it might be a good idea to choose something like:
        # <environment>_<splunksite>_<location>
        prod_site1_de:
            # the api server we connect to (should be the datacenter usually):
            api_host: "host-FQDN"
            api_user: "{{ vault_proxmox_api_user }}"
            api_token_id: "{{ vault_proxmox_api_token_id }}"
            api_token_secret: "{{ vault_proxmox_api_token_secret }}"
            validate_certs: yes
            # proxmox CT template including storage location.
            # usually you will use ansible_template_name instead
            #ostemplate: "ovh-nfs:vztmpl/opensuse-15.2-default_20200824_amd64.tar.xz"
            # proxmox server template used to clone a new machine:
            ansible_template_name: "suse-ansible-template"
            # the default storage where new VMs should be deployed:
            storage: <storage-name>

            # The community proxmox module is very limited in what you can actually do with cloud-init.
            # If you want to have full flexibility you can use custom ci files (https://cloudinit.readthedocs.io/en/latest/reference/examples.html)
            # PORS will:
            # - generate them during the deployment (based on templates in roles/proxmox/templates/ci_*.j2
            # - upload them to the Proxmox server
            # - setup the cloud-init image on the target host so when the VM starts up it gets fully configured by them
            #advanced_ci:
              #enabled: true

              # it is highly recommended to use a dedicated storage for the ci templates. this storage should be used for this purpose only.
              # required types for that storage: "Container template, Snippets"
              # Unfortunaley the Proxmox API does not allow to upload snippets directly so PORS will use a hackish way to upload them
              # (as a "Container template" including tar.gz ending - even though they are plain text files) to the templates directory.
              # The next limitation is that you cannot use cloud-init files from the templates directory so you need to prepare your
              # storage that they are linked. The following 2 steps are required to achieve this:
              # 1) Add the storage in the Proxmox UI:
              #    Storage -> Add -> Directory (or anything else allowing snippets+templates)
              #    -> select just "Snippets" + "Container template" and add a local <storage-path>
              # 2) Symlink templates with snippets directory:
              #    cicustom HAS TO be in SNIPPETS dir. the following works ONLY if you SYMLINK
              #    "<storage-path>/templates/cache" with "<storage-path>/snippets" (no sub dirs allowed)
              #    Note: if you are not quick enough the snippets dir will be auto-recreated by
              #    Proxmox but something like this works fine:
              #    "rm -rf <storage-path>/snippets; ln -s template/cache <storage-path>/snippets"
              # WARNING: do not execute the above blindly it will delete everything in your snippets dir so be
              # sure to use the right one (a dedicated storage is recommended as in step 1 above)
              # WARNING2: you should have got an understanding that this whole process is not supported by Proxmox
              # and can stop working at any time without any warnings.
              #
              # Finally add your Proxmox storage name here:
              #storage: automation_storage

              # system locale
              #locale: de_DE

              # system timezone
              #timezone: Europe/Berlin

              # extending the roles/proxmox/templates/ci_*.j2 with whatever you like
              # (https://cloudinit.readthedocs.io/en/latest/reference/examples.html)
              # this becomes part of the roles/proxmox/templates/ci_user.j2 (added at the end)
              #ciuser: |
              #   <ci config>, e.g:
              #   runcmd:
              #     - [ echo, "foo", ">", /moo.txt ]
              # this becomes part of the roles/proxmox/templates/ci_network.j2 (added at the end)
              #cinetwork:

        #staging_site1_us:
            #api_host: "host-FQDN"
            #....
 
