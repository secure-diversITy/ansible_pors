---
####################################################################################################################
#
# ALL OUTCOMMENTED KEY:VALUES (IF ANY) YOU SEE HERE ARE THE DEFINED DEFAULT VALUES
# i.e. there is no need to enable them if you like the shown default value
#
####################################################################################################################

#####################################################################################################################
# Deploy VM's based on a given template.
# Check documentation.
#####################################################################################################################

##############################
# VMware vSphere definitions #
##############################
vsphere:
    # if set to true no RAM backup swap (vswp) will be created
    # use this with care and avoid any over-provisioning of RAM then!
    # if not set or set to false vmem will act as usual.
    disablevmem: true

    # VMware will try to boot over PXE on each boot which is usually useless
    # set this to True to disable PXE boot when deploying a new VM
    disablepxeboot: True
    
    host:
       # you can specify as much vsphere centers as you wish.
       # vsphere-hostname-01 is just an example any should be replaced by the hostname
       # of your real FQDN (i.e. without domain).
       vsphere-hostname-01:
            dest: "full-fqdn"
            validate_certs: "no"
            user: "{{ vault_vsphere_user }}"
            password: "{{ vault_vsphere_password }}"
            ansible_template_name: "rhel_ansible_template"

       #vsphere-hostname-02:
           #dest: "full-fqdn"
            #...

##############################
# Google GCP definitions     #
##############################
googlecp:
    project:
        # replace <prod_site1_us> with a freely choosen name and "<my-project-id>" with the REAL project-id!
        # while you're free to choose any name it might be a good idea to choose something like:
        # <environment>_<splunksite>_<location>
        # rule of thumb is to split by service account, project id, metadata, region, zone, source image
        staging_site1_us:
            id: <my-project-id>
            cred_kind: serviceaccount
            cred_file: $HOME/.gcp.json
            zone: "us-central1-a"
            region: "us-central1"
            scopes:
                - https://www.googleapis.com/auth/compute
            source_image: "projects/templates/global/images/sles12-sp4-latest"
            metadata_service: "splunk"
            metadata_landscape: "staging"
            subnet: "<network-subnet-name-as-defined-in-GCP>"

        #staging_site2_eu:
        #    id: <my-project-id>
        #    ....

##############################
# Proxmox PVE definitions    #
##############################
proxmox:
    # datacenter node
    dc: 
        host: "host-FQDN"
        validate_certs: yes
    node:
        # replace <prod_site1_de> with the actual node name of your Proxmox VE
        # while you're free to choose any name it might be a good idea to choose something like:
        # <environment>_<splunksite>_<location>
        prod_site1_de:
            # the api server we connect to (should be the datacenter usually):
            api_host: "host-FQDN"
            api_user: "{{ vault_proxmox_api_user }}"
            api_token_id: "{{ vault_proxmox_api_token_id }}"
            api_token_secret: "{{ vault_proxmox_api_token_secret }}"
            validate_certs: yes
            # proxmox CT template including storage location.
            # usually you will use ansible_template_name instead
            #ostemplate: "ovh-nfs:vztmpl/opensuse-15.2-default_20200824_amd64.tar.xz"
            # proxmox server template used to clone a new machine:
            ansible_template_name: "suse-ansible-template"
            # the default storage where new VMs should be deployed:
            storage: <storage-name>

            # The community proxmox module is very limited in what you can actually do with cloud-init.
            # If you want to have full flexibility you can use custom ci files (https://cloudinit.readthedocs.io/en/latest/reference/examples.html)
            # PORS will:
            # - generate them during the deployment (based on templates in roles/proxmox/templates/ci_*.j2
            # - upload them to the Proxmox server
            # - setup the cloud-init image on the target host so when the VM starts up it gets fully configured by them
            #advanced_ci:
              #enabled: true

              # it is highly recommended to use a dedicated storage for the ci templates. this storage should be used for this purpose only.
              # required types for that storage: "Container template, Snippets"
              # Unfortunaley the Proxmox API does not allow to upload snippets directly so PORS will use a hackish way to upload them
              # (as a tar.gz template - even though they are plain text files) to the templates directory.
              # BUT: cicustom HAS TO be in SNIPPETS dir. the following works ONLY when you SYMLINK "templates/cache" with "snippets" (no sub dir allowed!!!).
              # Note: if you are not quick enough the snippets dir will be auto-recreated by Proxmox but something like this works fine:
              # "rm /var/lib/vz/snippets -rf; ln -s /var/lib/vz/template/cache /var/lib/vz/snippets" (for the local storage, replace that with yours!!!)
              # do not execute the above blindly it will delete everything in your snippets dir so be sure to use the right one (a dedicated storage is
              # recommended anyways)
              # type in the name of your Proxmox storage name:
              #storage: automation_storage

              # extending the roles/proxmox/templates/ci_*.j2 with whatever you like
              # (https://cloudinit.readthedocs.io/en/latest/reference/examples.html)
              # this becomes part of the roles/proxmox/templates/ci_user.j2 (added at the end)
              #ciuser:
              # this becomes part of the roles/proxmox/templates/ci_network.j2 (added at the end)
              #cinetwork:

        #staging_site1_us:
            #api_host: "host-FQDN"
            #....
 
